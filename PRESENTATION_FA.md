# متن ارائه پروژه — تحلیل رفتار مشتری و پیش‌بینی ریزش

**مخاطب:** مدیر بخش  
**هدف:** آشنایی کامل با سناریو، سوالات، پاسخ‌ها، روش‌های استفاده‌شده و دلیل عدم استفاده از روش‌های رقیب  
**مرجع سناریو:** `test.md`

---

## ۱. معرفی پروژه

این پروژه بر اساس سناریوی تعریف‌شده در **فایل test.md** طراحی و پیاده‌سازی شده است.

**سناریو (طبق test.md):**  
یک فروشگاه آنلاین کوچک فرضی که محصولات دست‌ساز می‌فروشد؛ منابع محدود و داده‌های مشتریان پراکنده و ناقص است.

**هدف پروژه:**  
با همین داده‌های محدود و نویزی، بینش عملی برای کسب‌وکار استخراج کنیم و ریزش مشتری را پیش‌بینی کنیم.

**خلاصه سناریو و هدف در کجا آمده است:**  
در **README.md** (بخش Scenario) و در **test.md** (ابتدای فایل).

---

## ۲. سوالات کلیدی و پاسخ کامل

طبق **test.md** چهار سوال اصلی مطرح شده است. پاسخ هر کدام در ادامه آمده است؛ در هر بخش روش استفاده‌شده، دلیل آن و دلیل عدم استفاده از روش‌های رقیب هم توضیح داده می‌شود.

---

### سوال ۱: چه عواملی روی علاقه مشتریان به محصولات ما تأثیر می‌گذارد؟

**پاسخ:**

- **احساسات (Sentiment):**  
  با تحلیل احساسات روی متن بازخوردها می‌فهمیم کدام نظرات منفی و کدام مثبت‌اند؛ امتیاز منفی یعنی احتمال نارضایتی و نیاز به پیگیری.

- **کلمات کلیدی:**  
  پرتکرارترین کلمات (مثل محصول، ارسال، طراحی) استخراج می‌شوند؛ نشان می‌دهند مشتریان روی چه موضوعاتی حساس‌اند.

- **منبع داده:**  
  با مقایسه دادهٔ داخلی (بازخورد مستقیم) و دادهٔ عمومی (مثل دیتاست احساسات) ترکیب هر دو بینش بهتری می‌دهد.

**روش استفاده‌شده و محل پیاده‌سازی:**  
تحلیل احساسات با
VADER
(در فایل
scripts/transform.py
داخل تابع
add_text_features
).  
کلمات کلیدی با تابع
keyword_frequency
در
scripts/eda.py
.  
مقایسه منابع با نمودار
source_type_counts
در
scripts/eda.py
.  
خلاصه در **SUMMARY.md** (Key Insights) و در نوت‌بوک **notebooks/01_full_pipeline.ipynb** (بخش Practical insights).

**چرا این روش‌ها:**  
VADER داخل NLTK است، بدون نیاز به GPU، و برای متن کوتاه و محاوره‌ای (نظرات) مناسب است.  
کلمات کلیدی با شمارش ساده و سریع به‌دست می‌آیند و برای دادهٔ کم کافی‌اند.

**چرا روش رقیب استفاده نشد:**  
برای احساسات، مدل‌های بزرگتر (مثل Hugging Face) دقیق‌ترند ولی برای دیتاست کوچک و اجرای سبک، VADER کافی است؛ مدل Hugging Face به‌صورت اختیاری در
scripts/models.py
در تابع
run_pretrained_sentiment_if_available
در دسترس است اگر نصب باشد.

---

### سوال ۲: چطور مشتریان بالقوه را پیدا کنیم؟

**پاسخ:**

- **بخش‌بندی (Segmentation):**  
  با **K-Means** مشتریان به چند گروه (کم‌خرید، میان‌خرید، پرخرید) تقسیم می‌شوند؛ گروه‌هایی که هنوز خرید کمی دارند یا از کانال‌های خاص می‌آیند می‌توانند مشتری بالقوه باشند.

- **منابع عمومی:**  
  پست‌های Reddit و محتوای وب افرادی را نشان می‌دهند که دربارهٔ محصولات دست‌ساز حرف می‌زنند؛ مخاطب بالقوه برای بازاریابی.

**روش استفاده‌شده و محل پیاده‌سازی:**  
بخش‌بندی با
K-Means
در
scripts/models.py
(تابع
run_segmentation
).  
خروجی در فایل
data/processed/customers_with_segments.csv
و نمودار
data/processed/plots/segment_distribution.png
.  
استخراج Reddit و وب در **scripts/extract_reddit_api.py** و **scripts/extract_web.py**.  
جزئیات در **SUMMARY.md** (Customer segments).

**چرا K-Means:**  
ساده، سریع و تعداد خوشه را ما تعیین می‌کنیم؛ برای «۲ یا ۳ خوشهٔ قابل تفسیر» ایده‌آل است و **test.md** صریحاً آن را خواسته است.

**چرا روش رقیب استفاده نشد:**  
روش‌هایی مثل
DBSCAN
یا خوشه‌بندی سلسله‌مراتبی یا نیاز به تنظیم پارامتر بیشتر دارند یا تفسیر خوشه‌ها سخت‌تر است؛ برای این سناریو
K-Means
کافی بود.

---

### سوال ۳: چه پیام‌های بازاریابی برای بخش‌های مختلف مشتری مؤثرتر است؟

**پاسخ:**

- **تفکیک بر اساس بخش:**  
  برای بخش با ارزش بالا (پرخرید) پیام حفظ و وفاداری؛ برای بخش متوسط پیام ارتقا (upsell)؛ برای بخش کم‌خرید یا بدون خرید اخیر پیام بازگرداندن (reactivation).

- **تفکیک بر اساس احساسات:**  
  برای کسانی که اخیراً نظر منفی داده‌اند نباید فشار بازاریابی زیاد باشد؛ اول رفع نارضایتی، بعد پیشنهاد جدید.

- **محتوا:**  
  با توجه به کلمات کلیدی پرتکرار (کیفیت، طراحی، ارسال) پیام‌ها حول همین موضوعات طراحی شوند.

**محل پیاده‌سازی و مستندات:**  
این توصیه‌ها از خروجی بخش‌بندی و احساسات استخراج شده و در **SUMMARY.md** (Practical Recommendations – Marketing و Content) نوشته شده‌اند.

---

### سوال ۴: چطور ریزش مشتری را با داده‌های محدود پیش‌بینی کنیم؟

**چرا این سوال مهم است:**  
شناسایی زودهنگام مشتریانی که در شرف ترک کردن هستند به کسب‌وکار اجازه می‌دهد با اقدام هدفمند (تخفیف، پیام شخصی‌سازی‌شده یا تماس) جلوی ریزش را بگیرد. با دادهٔ محدود باید تعریف ساده و مدل‌های تفسیرپذیر انتخاب کنیم تا هم نتیجه قابل استفاده باشد هم بتوان توضیح داد «چرا» این مشتری پرخطر است.

---

**۱. تعریف عملیاتی ریزش (Churn)**

در این پروژه ریزش به‌صورت **عملیاتی** این‌طور تعریف شده است:

- تاریخ مرجع = آخرین سفارشی که در کل دیتاست ثبت شده (ماکسیمم
  order_date
  ).
- پنجرهٔ ریزش = ۱۲۰ روز قبل از این تاریخ.
- اگر **آخرین سفارش** یک مشتری قبل از شروع این پنجره باشد، آن مشتری **ریزش‌شده** است (برچسب
  churn = 1
  ).

یعنی: مشتری که در ۱۲۰ روز منتهی به «امروزِ دیتاست» هیچ خریدی نکرده، ریزش‌شده در نظر گرفته می‌شود. عدد ۱۲۰ روز در کد به‌صورت پارامتر
last_days
در تابع
build_churn_dataset
قابل تغییر است و با چرخهٔ خرید کسب‌وکار (مثلاً فصلی یا ماهانه) قابل تنظیم است.

**محل پیاده‌سازی:**  
تابع
build_churn_dataset
در فایل
scripts/models.py
؛ ورودی: جدول مشتریان و سفارشات پاک‌شده؛ خروجی: یک جدول با یک ردیف به ازای هر مشتری به‌همراه برچسب
churn
.

---

**۲. ویژگی‌های ورودی مدل (Feature Engineering)**

برای پیش‌بینی ریزش از ویژگی‌هایی استفاده شده که با دادهٔ محدود قابل محاسبه‌اند و از نظر کسب‌وکاری معنادار هستند:

- **order_count:** تعداد کل سفارش‌های مشتری؛ کمتر بودن اغلب با ریزش بیشتر همراه است.
- **total_amount:** مجموع مبلغ خرید؛ مشتریان با ارزش بالاتر معمولاً کمتر ریزش می‌کنند.
- **day_of_week، month، season:** الگوی زمانی و اثر فصلی (فصل به‌صورت عدد ۱ تا ۴).
- **channel_code، region_code:** کانال جذب و منطقهٔ مشتری به‌صورت کد عددی (Categorical encoding).

لیست نهایی ویژگی‌ها در تابع
get_churn_features_target
در
scripts/models.py
تعیین می‌شود. قبل از آموزش، ویژگی‌ها با
StandardScaler
نرمال می‌شوند.

---

**۳. مدل‌های استفاده‌شده و نحوهٔ آموزش**

دو مدل **طبقه‌بندی دودویی** (ریزش / عدم ریزش) آموزش داده می‌شوند:

- **Logistic Regression:** مدل خطی؛ خروجی احتمال ریزش؛ **ضرایب** هر ویژگی مستقیم قابل خواندن است و می‌گوید آن ویژگی چقدر ریزش را افزایش یا کاهش می‌دهد.
- **Naive Bayes (گوسی):** مدل احتمالاتی ساده؛ فرض استقلال ویژگی‌ها؛ در دادهٔ کم و نویزی اغلب پایدار عمل می‌کند.

آموزش و ارزیابی در تابع
train_churn_models
در
scripts/models.py
انجام می‌شود. مدل‌های نهایی (به‌همراه
scaler
) در پوشه
data/processed/models/
در فایل‌های
churn_logistic.joblib
و
churn_naive_bayes.joblib
ذخیره می‌شوند.

---

**۴. ارزیابی: چرا Cross-Validation و چرا Precision / Recall / F1**

- **K-Fold Cross-Validation (۳-fold):** با دادهٔ کم، یک بار تقسیم train/test ممکن است تصادفی باشد. با
  K-Fold
  چند بار ارزیابی انجام می‌شود و میانگین معیارها پایدارتر است. ۳-fold انتخاب شده تا در هر بار به‌اندازهٔ کافی نمونه برای آموزش باقی بماند (با ۵ یا ۱۰ fold در دادهٔ خیلی کم هر fold خیلی کوچک می‌شود).

- **Precision، Recall و F1 به‌جای فقط Accuracy:** در مسئلهٔ ریزش معمولاً کلاس «ریزش‌شده» **اقلیت** است. اگر مدل همه را «عدم ریزش» پیش‌بینی کند، دقت عددی بالا می‌رود ولی از نظر کسب‌وکاری بی‌فایده است.  
  **Precision** = از میان کسانی که مدل «ریزش» پیش‌بینی کرده، چند نفر واقعاً ریزش‌شده‌اند.  
  **Recall** = از همهٔ ریزش‌شدگان واقعی، چند نفر را مدل درست شناسایی کرده.  
  **F1** = میانگین هم‌ساز Precision و Recall.  
  این سه معیار عملکرد مدل روی کلاس اقلیت و تعادل بین مثبت کاذب و منفی کاذب را شفاف می‌کنند.

خروجی ارزیابی (
classification_report
،
confusion_matrix
،
precision
،
recall
،
f1
) در خروجی
train_churn_models
برمی‌گردد و در
SUMMARY.md
بخش Churn prediction خلاصه می‌شود.

---

**۵. تفسیر نتایج و استفادهٔ عملی**

برای **رگرسیون لجستیک** یک دیکشنری
feature_importance
ساخته می‌شود: به ازای هر ویژگی، ضریب مدل ذخیره می‌شود. ضریب **مثبت** یعنی با افزایش آن ویژگی، احتمال ریزش بیشتر می‌شود؛ ضریب **منفی** یعنی با افزایش آن ویژگی، احتمال ریزش کمتر می‌شود. مثلاً اگر
order_count
ضریب منفی داشته باشد یعنی «هرچه تعداد سفارش بیشتر، ریزش کمتر» که با انتظار کسب‌وکاری هم‌خوان است. این تفسیر به مدیر و تیم بازاریابی کمک می‌کند بفهمند روی چه عواملی باید تمرکز کنند (مثلاً تشویق به خرید مجدد برای کسانی که تعداد سفارش کم دارند).

---

**۶. چرا این روش‌ها و چرا روش‌های رقیب استفاده نشد**

| انتخاب | دلیل |
|--------|------|
| تعریف ریزش با ۱۲۰ روز | قابل تنظیم است (پارامتر
last_days
)؛ با چرخهٔ خرید کسب‌وکار هماهنگ می‌شود. |
| Logistic Regression | **test.md** صریحاً برای ریزش با دادهٔ کم پیشنهاد کرده؛ تفسیرپذیری بالا؛ با دادهٔ کم
overfitting
کمتر از مدل‌های پیچیده. |
| Naive Bayes در کنار آن | **test.md** سادگی و مقاومت در برابر نویز را خواسته؛ مقایسهٔ دو مدل برای اطمینان از پایداری نتیجه. |
| عدم استفاده از Random Forest / XGBoost | با حجم کم داده این مدل‌ها راحت‌تر
overfit
می‌کنند؛ تفسیر «کدام ویژگی چقدر اثر دارد» سخت‌تر است؛ سناریو بر تفسیرپذیری تأکید دارد. |
| ۳-fold به‌جای ۵ یا ۱۰ | با دادهٔ خیلی کم، foldهای بیشتر یعنی هر fold کوچک‌تر؛ ۳-fold تعادل بین پایداری و حجم آموزش است. |

---

**جمع‌بندی سوال ۴:**  
ریزش با تعریف عملیاتی «عدم خرید در ۱۲۰ روز» و با ویژگی‌های سادهٔ قابل محاسبه از همان دادهٔ محدود مدل شده است. دو مدل تفسیرپذیر (Logistic Regression و Naive Bayes) با
Cross-Validation
و معیارهای
Precision
،
Recall
و
F1
ارزیابی می‌شوند؛ ضرایب رگرسیون لجستیک برای اقدام عملی (تمرکز روی چه عواملی) استفاده می‌شود و مدل‌ها در
data/processed/models/
برای استفادهٔ بعدی ذخیره شده‌اند.

---

## ۳. معماری پروژه (خلاصه)

- **config.py:**  
  مسیرها و بارگذاری کلیدهای API از `.env` (مثلاً برای Reddit).

- **scripts:**  
  هر منبع داده یک اسکریپت جدا دارد تا پایپ‌لاین انعطاف‌پذیر و قابل نگهداری باشد.

- **data/raw:**  
  دادهٔ خام (CSV و JSON).

- **data/processed:**  
  دادهٔ پاک‌شده، منبع واحد حقیقت، نمودارها و مدل‌ها.

ساختار کامل در **README.md** (Repository Structure) و **PROJECT_GUIDE_FA.md** (جدول ساختار پوشه‌ها) است.

---

## ۴. استخراج داده — روش‌ها و دلیل انتخاب

**چه کردیم:**  
برای هر منبع (داخلی، احساسات عمومی، وب، Reddit، CSV دلخواه) یک اسکریپت جدا نوشته شد؛ خروجی در **data/raw** به صورت CSV یا JSON ذخیره می‌شود.  
مرجع: **scripts/extract_synthetic.py**, **extract_public_dataset.py**, **extract_web.py**, **extract_reddit_api.py**, **extract_from_csv.py** و **PROJECT_GUIDE_FA.md** (بخش استخراج).

**چرا اسکریپت جدا برای هر منبع:**  
اگر یک منبع خطا بدهد یا موقتاً قطع شود، بقیهٔ پایپ‌لاین تحت تأثیر مستقیم قرار نمی‌گیرد؛ تست و تغییر هر منبع راحت‌تر است. مطابق **test.md**: «اسکریپت‌های پایتون کوچک و قابل مدیریت برای هر منبع».

**چرا برای وب از BeautifulSoup استفاده شد:**  
سبک و بدون وابستگی سنگین؛ برای استخراج متن از چند صفحه کافی است.

**چرا از Scrapy استفاده نشد:**  
برای پروژهٔ کوچک و تعداد کم
URL
لازم نبود و پیچیدگی اضافه می‌کرد.

**چرا برای Reddit هم API و هم دادهٔ نمونه:**  
با کلید API در `.env` داده واقعی از Reddit گرفته می‌شود. بدون کلید، تابع
sample_reddit_like_data
در
scripts/extract_reddit_api.py
پست‌های نمونه ذخیره می‌کند تا کل پایپ‌لاین بدون وابستگی به شبکه هم قابل اجرا باشد (جمع‌آوری آفلاین طبق **test.md**).

---

## ۵. تبدیل و پاکسازی — روش‌ها و دلیل انتخاب

همهٔ منطق در **scripts/transform.py** است؛ جزئیات توابع در **PROJECT_GUIDE_FA.md** (بخش تبدیل).

**دادهٔ عددی گمشده — چرا میانه (median) نه میانگین (mean):**  
در دادهٔ نویزی مقدارهای افراطی زیاد هستند؛ میانگین به‌راحتی منحرف می‌شود. میانه در برابر این مقادیر مقاوم‌تر است. در کد با تابع
fill_missing_numeric
و پارامتر
strategy="median"
.

**حذف ردیف‌های ناقص — چرا با «احتیاط» و با min_fill_ratio:**  
حذف هر ردیفی که حتی یک مقدار خالی دارد باعث از دست رفتن زیاد داده می‌شود. تابع
drop_rows_missing_key_columns
فقط ردیف‌هایی را حذف می‌کند که نسبت ستون‌های کلیدی پر شده‌شان از حد معین (مثلاً ۰.۵) کمتر باشد. مطابق **test.md**: «حذف ردیف‌هایی که اطلاعات کلیدی‌شان را ندارند (با احتیاط)».

**متن — چرا Stemming به‌صورت پیش‌فرض و Lemmatization اختیاری:**  
Stemming (Porter) سریع و بدون نیاز به دیکشنری؛ برای متن کوتاه و نظرات کافی است. Lemmatization (WordNet) دقیق‌تر ولی کندتر؛ به‌صورت اختیاری در تابع
transform_feedback
با پارامتر
use_lemmatize=True
قابل استفاده است. **test.md** هر دو را مجاز دانسته؛ پیش‌فرض Stemming برای اجرای سبک‌تر انتخاب شد.

**احساسات متن — چرا VADER و Hugging Face اختیاری:**  
VADER داخل NLTK، بدون GPU، مناسب متن کوتاه و محاوره‌ای. Hugging Face (مدل از پیش‌آموزش‌دیده) دقیق‌تر ولی نیاز به نصب transformers و حافظهٔ بیشتر؛ در تابع
run_pretrained_sentiment_if_available
در
scripts/models.py
در صورت نصب استفاده می‌شود. مطابق **test.md** برای احساسات در صورت نداشتن دیتاست بزرگ، مدل از پیش‌آموزش‌دیده پیشنهاد شده؛ هر دو پوشش داده شده‌اند.

**چرا یک Single Source of Truth:**  
همهٔ دادهٔ پاک‌شده (بازخورد داخلی + احساسات عمومی) در یک جدول واحد جمع می‌شود تا تحلیل و مدل‌سازی فقط روی یک منبع انجام شود. خروجی در فایل‌های
data/processed/single_source_of_truth.csv
و
single_source_of_truth.parquet
.

---

## ۶. مدل‌سازی — روش‌ها و دلیل انتخاب

همهٔ مدل‌ها در **scripts/models.py**؛ خلاصه در **SUMMARY.md** (Key Insights) و **CHECKLIST_TEST_MD.md** (Phase 4).

**ارزیابی — چرا K-Fold Cross-Validation و چرا ۳-fold:**  
با دادهٔ کم، یک تقسیم train/test ثابت ممکن است تصادفی باشد؛ با CV چند بار ارزیابی می‌شود و نتیجه پایدارتر است. ۳-fold انتخاب شده تا در هر بار به‌اندازهٔ کافی داده برای آموزش باقی بماند؛ با ۵ یا ۱۰ fold در دادهٔ خیلی کم هر fold خیلی کوچک می‌شود.

---

## ۷. تحلیل اکتشافی و بصری‌سازی

همه در **scripts/eda.py**؛ نمودارها در **data/processed/plots/** و توضیح هر نمودار در **SUMMARY.md** (جدول Key Visualizations).

- **هیستوگرام:** توزیع احساسات و طول متن (تعداد کلمات).
- **نمودار میله‌ای:** مقایسه منبع داده، تعداد بازخورد به تفکیک ماه، توزیع بخش‌ها.
- **نمودار خطی:** روند تعاملات در زمان.
- **پراکندگی (scatter):** رابطهٔ طول متن با امتیاز احساسات.
- **جعبه‌ای (boxplot):** شناسایی نویز و مقدارهای افراطی در احساسات.

**test.md** همین نوع نمودارها را خواسته و استفاده از آن‌ها برای شناسایی نویز. تابع
get_practical_insights
در
scripts/eda.py
پاسخ سوالات عملی (موضوعات، احساس، الگوی زمانی، بهترین منبع) را می‌سازد.

---

## ۸. گزارش زنده و مستندات

- **notebooks/01_full_pipeline.ipynb:**  
  گزارش زنده از استخراج تا مدل و نمودارها و داستان پروژه (چالش‌ها، رویکرد، یافته‌ها، پیشنهادات).

- **README.md:**  
  سناریو، ساختار، دستورات اجرا، چالش‌ها، یافته‌ها، محدودیت‌ها.

- **SUMMARY.md:**  
  بینش‌های کلیدی، توصیه‌های عملی، محدودیت‌ها، جدول نمودارهای کلیدی (مطابق خواستهٔ test.md برای «فایل خلاصه»).

- **PROJECT_GUIDE_FA.md:**  
  راهنمای فنی کامل به فارسی (توابع، پارامترها، مسیر فایل‌ها).

- **CHECKLIST_TEST_MD.md:**  
  تطابق هر الزام test.md با پیاده‌سازی (فایل/تابع).

---

## ۹. جمع‌بندی برای مدیر

- **مسئله و سوالات:** در **test.md** تعریف شده‌اند؛ پاسخ هر سوال در تحلیل و مدل‌ها پیاده شده و در **SUMMARY.md** و نوت‌بوک خلاصه شده است.

- **داده:** استخراج از چند منبع با اسکریپت‌های جدا؛ ذخیرهٔ استاندارد (CSV, JSON) برای استفادهٔ آفلاین.

- **پاکسازی:** پاکسازی تکراری، میانه برای عددی، حذف احتیاطی ردیف، Stemming و VADER و در صورت نیاز Lemmatization و مدل Hugging Face؛ یک منبع واحد حقیقت.

- **مدل‌سازی:** ریزش با دو مدل تفسیرپذیر (Logistic Regression و Naive Bayes) و مقاوم در برابر نویز؛ بخش‌بندی با K-Means؛ ارزیابی با
  CV
  و
  Precision/Recall/F1
  ؛ ذخیرهٔ مدل‌ها در پوشه
  data/processed/models/
  برای استفادهٔ بعدی.

- **مستندات:** README، SUMMARY، راهنمای فارسی (PROJECT_GUIDE_FA.md)، چک‌لیست تطابق با test.md و نوت‌بوک گزارش زنده.

با مطالعهٔ این متن و در صورت نیاز مراجعه به فایل‌های نام‌برده شده، تصویر کامل از سناریو، روش‌ها، دلیل انتخاب‌ها، دلیل عدم استفاده از روش‌های رقیب و پاسخ سوالات برای مدیر بخش فراهم می‌شود.

---

*منبع: test.md, README.md, SUMMARY.md, PROJECT_GUIDE_FA.md, CHECKLIST_TEST_MD.md و کد در scripts/ و notebooks/01_full_pipeline.ipynb.*
